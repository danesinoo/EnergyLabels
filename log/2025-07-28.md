---
title: Binary Classifier
author: "Carlo Rosso"
date: 2025-07-28
---

As a binary classifier I wanted to use `AutoModelForSequenceClassification`, but
it isn't working on my PC, because I don't have enough RAM: I think that I need
about 20GB of RAM, up to 40GB, using Qwen0.6B.
I already did it on my Bachelor's thesis and I know that BERT takes about 6
hours. So I think it is still not feasible.
Notably, BERT is somewhat smaller than Qwen0.6B, but Qwen0.6B at usage is more
optimized and thus it requires less resources.

What I am going to do is the following:
1. Qwen's tokenizer.
2. Qwen to produce the embeddings.
3. RNN to compute one embedding from a list of embedding (the sentence).
4. NN to predict the class from the final embedding.

Notably, 1. and 2. do not need any training. And RNN and NN are quite fast to
train.

In 3. we can think to use [[https://github.com/alxndrTL/mamba.py]] for speed up,
but my pc does not have GPU, thus I can't leverage on it.

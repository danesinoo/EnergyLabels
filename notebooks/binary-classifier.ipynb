{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2341187",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a48b3fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from tqdm.notebook import trange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b07e1420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SerialID</th>\n",
       "      <th>building_id</th>\n",
       "      <th>zone_id</th>\n",
       "      <th>SEEB</th>\n",
       "      <th>S_text</th>\n",
       "      <th>L_text</th>\n",
       "      <th>Type</th>\n",
       "      <th>Pieces1</th>\n",
       "      <th>Manufacturer1</th>\n",
       "      <th>SubType1</th>\n",
       "      <th>...</th>\n",
       "      <th>SpecifiedSubscriptionUnitAmount</th>\n",
       "      <th>DistrictHeatingPlantName</th>\n",
       "      <th>HeatLoss</th>\n",
       "      <th>BFactor</th>\n",
       "      <th>NominalEffect</th>\n",
       "      <th>MinimumTemperature</th>\n",
       "      <th>StandbyPowerUsage</th>\n",
       "      <th>HeatsHotWater</th>\n",
       "      <th>text</th>\n",
       "      <th>SingleHx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>311117034</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2-1-3-0</td>\n",
       "      <td>Fjernvarme med isoleret veksler (indirekte anl...</td>\n",
       "      <td>Bygningen opvarmes med fjernvarme. Anlægget er...</td>\n",
       "      <td>DistrictHeatWithExchanger</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>HOFOR (Københavns Energi) - MWh (04-07-2014)</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Fjernvarme med isoleret veksler (indirekte anl...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>311117893</td>\n",
       "      <td>2050872</td>\n",
       "      <td>2050872</td>\n",
       "      <td>2-1-3-0</td>\n",
       "      <td>Eksisterende fjernvarme</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DistrictHeatWithExchanger</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Københavns Energi MWh (293)</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Eksisterende fjernvarme.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>311119019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2-1-3-0</td>\n",
       "      <td>Fjernvarme med uisoleret veksler (indirekte an...</td>\n",
       "      <td>Bygningen opvarmes med fjernvarme. Anlægget er...</td>\n",
       "      <td>DistrictHeatWithExchanger</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>190.0</td>\n",
       "      <td>HOFOR (Københavns Energi) - MWh (04-07-2014)</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Fjernvarme med uisoleret veksler (indirekte an...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>311121828</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2-1-3-0</td>\n",
       "      <td>Fjernvarme med isoleret veksler (indirekte anl...</td>\n",
       "      <td>Ejendommen opvarmes med fjernvarme fra HOFOR.\\...</td>\n",
       "      <td>DistrictHeatWithExchanger</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Danfoss Redan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>502.0</td>\n",
       "      <td>HOFOR (Københavns Energi) - MWh (04-07-2014)</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Fjernvarme med isoleret veksler (indirekte anl...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>311122000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2-1-3-0</td>\n",
       "      <td>Fjernvarme med isoleret veksler (indirekte anl...</td>\n",
       "      <td>Bygningen opvarmes med fjernvarme. Anlægget er...</td>\n",
       "      <td>DistrictHeatWithExchanger</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>HOFOR (Københavns Energi) - MWh (04-07-2014)</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Fjernvarme med isoleret veksler (indirekte anl...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    SerialID  building_id  zone_id     SEEB  \\\n",
       "0  311117034            0        0  2-1-3-0   \n",
       "1  311117893      2050872  2050872  2-1-3-0   \n",
       "2  311119019            0        0  2-1-3-0   \n",
       "3  311121828            0        0  2-1-3-0   \n",
       "4  311122000            0        0  2-1-3-0   \n",
       "\n",
       "                                              S_text  \\\n",
       "0  Fjernvarme med isoleret veksler (indirekte anl...   \n",
       "1                            Eksisterende fjernvarme   \n",
       "2  Fjernvarme med uisoleret veksler (indirekte an...   \n",
       "3  Fjernvarme med isoleret veksler (indirekte anl...   \n",
       "4  Fjernvarme med isoleret veksler (indirekte anl...   \n",
       "\n",
       "                                              L_text  \\\n",
       "0  Bygningen opvarmes med fjernvarme. Anlægget er...   \n",
       "1                                                NaN   \n",
       "2  Bygningen opvarmes med fjernvarme. Anlægget er...   \n",
       "3  Ejendommen opvarmes med fjernvarme fra HOFOR.\\...   \n",
       "4  Bygningen opvarmes med fjernvarme. Anlægget er...   \n",
       "\n",
       "                        Type  Pieces1  Manufacturer1 SubType1  ...  \\\n",
       "0  DistrictHeatWithExchanger      1.0        Unknown      NaN  ...   \n",
       "1  DistrictHeatWithExchanger      1.0        Unknown      NaN  ...   \n",
       "2  DistrictHeatWithExchanger      1.0        Unknown      NaN  ...   \n",
       "3  DistrictHeatWithExchanger      1.0  Danfoss Redan      NaN  ...   \n",
       "4  DistrictHeatWithExchanger      1.0        Unknown      NaN  ...   \n",
       "\n",
       "  SpecifiedSubscriptionUnitAmount  \\\n",
       "0                            15.0   \n",
       "1                             0.0   \n",
       "2                           190.0   \n",
       "3                           502.0   \n",
       "4                            25.0   \n",
       "\n",
       "                       DistrictHeatingPlantName HeatLoss  BFactor  \\\n",
       "0  HOFOR (Københavns Energi) - MWh (04-07-2014)      1.1      0.0   \n",
       "1                   Københavns Energi MWh (293)      1.5      0.7   \n",
       "2  HOFOR (Københavns Energi) - MWh (04-07-2014)      5.0      0.0   \n",
       "3  HOFOR (Københavns Energi) - MWh (04-07-2014)      1.1      0.0   \n",
       "4  HOFOR (Københavns Energi) - MWh (04-07-2014)      7.0      0.0   \n",
       "\n",
       "  NominalEffect MinimumTemperature StandbyPowerUsage HeatsHotWater  \\\n",
       "0          20.0               60.0               0.0           1.0   \n",
       "1          16.0                0.0               5.0           0.0   \n",
       "2          20.0               65.0               0.0           0.0   \n",
       "3          60.0               60.0               0.0           0.0   \n",
       "4          20.0               65.0               0.0           0.0   \n",
       "\n",
       "                                                text  SingleHx  \n",
       "0  Fjernvarme med isoleret veksler (indirekte anl...      True  \n",
       "1                          Eksisterende fjernvarme.       True  \n",
       "2  Fjernvarme med uisoleret veksler (indirekte an...      True  \n",
       "3  Fjernvarme med isoleret veksler (indirekte anl...      True  \n",
       "4  Fjernvarme med isoleret veksler (indirekte anl...      True  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_gt = pd.read_excel(\"../data/data_district_heating.xlsx\", sheet_name=\"Ground Truth\")\n",
    "d_gt['text'] = d_gt['S_text'].fillna(\"\").astype(str) + \". \" + d_gt['L_text'].fillna(\"\").astype(str)\n",
    "d_gt['SingleHx'] = d_gt.iloc[:, 13:31].isna().all(axis=1)\n",
    "d_gt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08620d30",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473c55b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, tokens_list, labels_list):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tokens_list: List of token embeddings (each item is a tensor of shape [seq_len, embedding_dim])\n",
    "            labels_list: List of labels (each item is a tensor of shape [len(tokens_list), 1])\n",
    "        \"\"\"\n",
    "        self.tokens = tokens_list\n",
    "        self.labels = labels_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokens[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1513a4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, model_name=\"saattrupdan/nbailab-base-ner-scandi\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name\n",
    "        )\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize text and return their embeddings.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text to tokenize\n",
    "\n",
    "        Returns:\n",
    "            embeddings: List of tokens' embeddings\n",
    "        \"\"\"\n",
    "        # Get tokenized inputs and model outputs\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            output = self.model(**inputs).last_hidden_state.squeeze(0)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7b3fa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at saattrupdan/nbailab-base-ner-scandi and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36d18714f604133be9d2c1d333f01c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/2272 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model_name = \"Qwen/Qwen3-0.6B\"\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "X = []\n",
    "for i in trange(d_gt.shape[0], desc=\"Tokenizing\"):\n",
    "    X.append(tokenizer.tokenize(d_gt.loc[i, 'text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82a4e508",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = d_gt['SingleHx'].astype(int).tolist()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b2ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingRNNClassifier(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_size, output_size=1):\n",
    "        super().__init__()\n",
    "        # Custom RNN (using GRU for efficiency)\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True,\n",
    "            # bidirectional=False # default is unidirectional\n",
    "        )\n",
    "        \n",
    "        # Binary classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # Process embeddings sequentially with RNN\n",
    "        rnn_out, _ = self.rnn(embeddings)\n",
    "        \n",
    "        # Extract final timestep output (aggregated representation)\n",
    "        last_hidden = rnn_out[:, -1, :]\n",
    "        \n",
    "        # Binary classification\n",
    "        return self.classifier(last_hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35190f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7034f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLModel(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim=768,\n",
    "        hidden_size=14,\n",
    "        num_classes=1,\n",
    "        nn=EmbeddingRNNClassifier,\n",
    "        initial_batch_size=16,\n",
    "        max_batch_size=64,\n",
    "        num_epochs=1000,\n",
    "        lr=0.001,\n",
    "    ):\n",
    "        # Store all parameters as attributes (required for sklearn)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.nn = nn\n",
    "        self.initial_batch_size = initial_batch_size\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.lr = lr\n",
    "\n",
    "        # Internal attributes\n",
    "        self.model = None\n",
    "        self.is_fitted_ = False\n",
    "\n",
    "    def _initialize_model(self):\n",
    "        \"\"\"Initialize the neural network model\"\"\"\n",
    "        if self.nn is None:\n",
    "            raise ValueError(\"Neural network class (nn) must be provided\")\n",
    "        self.model = self.nn(self.embedding_dim, self.hidden_size, self.num_classes)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the token classifier.\n",
    "\n",
    "        Args:\n",
    "            X: list/array of tokens\n",
    "            y: labels\n",
    "        Returns:\n",
    "            self: Returns the instance itself\n",
    "        \"\"\"\n",
    "        self._initialize_model()\n",
    "        train_dataset = TensorDataset(X, y)\n",
    "\n",
    "        criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "        current_batch_size = self.initial_batch_size\n",
    "\n",
    "        print(f\"Starting training with batch size {current_batch_size}\")\n",
    "        self.is_fitted_ = True\n",
    "        pbar = trange(self.num_epochs)\n",
    "        for epoch in pbar:\n",
    "            train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=current_batch_size,\n",
    "                shuffle=True,\n",
    "                collate_fn=collate_fn,\n",
    "            )\n",
    "\n",
    "            self.model.train()\n",
    "            epoch_loss = 0.0\n",
    "            num_batches = 0\n",
    "\n",
    "            for tokens, labels, attention_mask in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                logits = self.model(tokens)\n",
    "\n",
    "                loss_per_token = criterion(logits, labels)\n",
    "                loss_per_token = loss_per_token * attention_mask.unsqueeze(-1)\n",
    "                loss = loss_per_token.sum() / attention_mask.sum()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "            avg_epoch_loss = epoch_loss / num_batches\n",
    "\n",
    "            pbar.set_postfix(\n",
    "                {\n",
    "                    \"Train Loss\": f\"{avg_epoch_loss:.4f}\",\n",
    "                    \"Batch Size\": f\"{current_batch_size}\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if avg_epoch_loss < 0.01 and current_batch_size < self.max_batch_size:\n",
    "                current_batch_size = min(current_batch_size * 2, self.max_batch_size)\n",
    "\n",
    "        self.training_results_ = {\n",
    "            \"train_losses\": train_losses,\n",
    "            \"train_accuracies\": train_accs,\n",
    "            \"model\": self.model,\n",
    "        }\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, batch_size=32):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "\n",
    "        Args:\n",
    "            X: list/array of tokens (same format as fit method)\n",
    "            batch_size: batch size for prediction (default: 32)\n",
    "\n",
    "        Returns:\n",
    "            list: Predicted labels for each token sequence\n",
    "        \"\"\"\n",
    "        if not self.is_fitted_:\n",
    "            raise ValueError(\n",
    "                \"This classifier has not been fitted yet. Call 'fit' first.\"\n",
    "            )\n",
    "\n",
    "        dummy_labels = [np.zeros_like(token_list) for token_list in X]\n",
    "        predict_dataset = TokenDataset(X, dummy_labels)\n",
    "\n",
    "        # Create data loader using the same collate_fn as in fit\n",
    "        predict_loader = DataLoader(\n",
    "            predict_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=collate_fn,\n",
    "        )\n",
    "\n",
    "        predictions = []\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for tokens, _, attention_mask in predict_loader:\n",
    "                logits = self.model(tokens)\n",
    "                pred = torch.argmax(logits, dim=-1)\n",
    "\n",
    "                for i in range(pred.size(0)):\n",
    "                    seq_pred = pred[i]\n",
    "                    seq_mask = attention_mask[i]\n",
    "                    valid_predictions = seq_pred[seq_mask.bool()].cpu().numpy().tolist()\n",
    "                    predictions.append(valid_predictions)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def score(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Return the mean accuracy on the given test data and labels.\n",
    "\n",
    "        Args:\n",
    "            X: Test samples\n",
    "            y: True labels\n",
    "\n",
    "        Returns:\n",
    "            float: Mean accuracy score\n",
    "        \"\"\"\n",
    "        if not self.is_fitted_:\n",
    "            raise ValueError(\n",
    "                \"This classifier has not been fitted yet. Call 'fit' first.\"\n",
    "            )\n",
    "\n",
    "        # Create evaluation dataset and loader\n",
    "        eval_dataset = TokenDataset(X, y)\n",
    "        eval_loader = DataLoader(\n",
    "            eval_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "        return self.eval(eval_loader)\n",
    "\n",
    "    def eval(self, eval_loader):\n",
    "        \"\"\"Evaluate the model on the given data loader\"\"\"\n",
    "        if not self.is_fitted_:\n",
    "            raise ValueError(\n",
    "                \"This classifier has not been fitted yet. Call 'fit' first.\"\n",
    "            )\n",
    "\n",
    "        acc_sum = 0.0\n",
    "        total_tokens = 0\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for tokens, labels, attention_mask in eval_loader:\n",
    "                logits = self.model(tokens)\n",
    "\n",
    "                pred = torch.argmax(logits, dim=-1)\n",
    "                target = torch.argmax(labels, dim=-1)\n",
    "\n",
    "                acc_per_token = (pred == target).float() * attention_mask\n",
    "                acc_sum += acc_per_token.sum().item()\n",
    "                total_tokens += attention_mask.sum().item()\n",
    "\n",
    "        acc = acc_sum / total_tokens if total_tokens > 0 else 0.0\n",
    "        return acc\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"\n",
    "        Get parameters for this estimator.\n",
    "\n",
    "        Args:\n",
    "            deep (bool): If True, return parameters for sub-estimators too.\n",
    "\n",
    "        Returns:\n",
    "            dict: Parameter names mapped to their values.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"embedding_dim\": self.embedding_dim,\n",
    "            \"num_classes\": self.num_classes,\n",
    "            \"nn\": self.nn,\n",
    "            \"initial_batch_size\": self.initial_batch_size,\n",
    "            \"max_batch_size\": self.max_batch_size,\n",
    "            \"num_epochs\": self.num_epochs,\n",
    "            \"eval_split\": self.eval_split,\n",
    "            \"seed\": self.seed,\n",
    "            \"lr\": self.lr,\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        \"\"\"\n",
    "        Set the parameters of this estimator.\n",
    "\n",
    "        Args:\n",
    "            **params: Estimator parameters.\n",
    "\n",
    "        Returns:\n",
    "            self: Estimator instance.\n",
    "        \"\"\"\n",
    "        valid_params = set(self.get_params().keys())\n",
    "        for key, value in params.items():\n",
    "            if key not in valid_params:\n",
    "                raise ValueError(\n",
    "                    f\"Invalid parameter {key} for estimator {type(self).__name__}\"\n",
    "                )\n",
    "            setattr(self, key, value)\n",
    "\n",
    "        # Reset fitted state when parameters change\n",
    "        self.is_fitted_ = False\n",
    "        self.model = None\n",
    "        self.training_results_ = None\n",
    "\n",
    "        return self\n",
    "\n",
    "    def save_model(self, file_name):\n",
    "        \"\"\"\n",
    "        Save the trained model to a file.\n",
    "\n",
    "        Args:\n",
    "            file_name (str): Path where to save the model\n",
    "        \"\"\"\n",
    "        if not self.is_fitted_:\n",
    "            raise ValueError(\"Cannot save model that hasn't been fitted yet.\")\n",
    "\n",
    "        try:\n",
    "            # Save the model state dict along with architecture parameters\n",
    "            save_dict = {\n",
    "                \"state_dict\": self.model.state_dict(),\n",
    "                \"embedding_dim\": self.embedding_dim,\n",
    "                \"num_classes\": self.num_classes,\n",
    "                \"nn_class\": self.nn.__name__ if self.nn else None,\n",
    "            }\n",
    "            torch.save(save_dict, file_name)\n",
    "            print(f\"Model saved successfully to {file_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def load_model(self, file_name, nn_class=None):\n",
    "        \"\"\"\n",
    "        Load a previously saved model.\n",
    "\n",
    "        Args:\n",
    "            file_name (str): Path to the saved model\n",
    "            nn_class: Neural network class to instantiate (optional if saved with model)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            save_dict = torch.load(file_name)\n",
    "\n",
    "            # Use saved architecture parameters or class parameters\n",
    "            embedding_dim = save_dict.get(\"embedding_dim\", self.embedding_dim)\n",
    "            num_classes = save_dict.get(\"num_classes\", self.num_classes)\n",
    "\n",
    "            if nn_class is None:\n",
    "                nn_class = self.nn\n",
    "            if nn_class is None:\n",
    "                raise ValueError(\n",
    "                    \"Neural network class must be provided either in constructor or as parameter\"\n",
    "                )\n",
    "\n",
    "            # Initialize model architecture\n",
    "            self.model = nn_class(embedding_dim, num_classes=num_classes)\n",
    "            # Load the saved state dict\n",
    "            self.model.load_state_dict(save_dict[\"state_dict\"])\n",
    "            self.model.eval()  # Set to evaluation mode\n",
    "            self.is_fitted_ = True\n",
    "            print(f\"Model loaded successfully from {file_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "  Model     raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "energylabels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
